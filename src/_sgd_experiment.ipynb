{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import DataPreparator\n",
    "from apply_clustering import ClusteringApplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from itertools import chain, combinations\n",
    "from typing import Dict, List, Tuple, Iterable, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gapstatistics.gapstatistics import GapStatistics\n",
    "from clustering import AgglomerativeClusteringWrapper as AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.spatial.distance import jensenshannon, correlation, euclidean\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "import seaborn as sns\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explainable: pd.DataFrame = ClusteringApplier.read_explaining_features()\n",
    "df_observable_distances: pd.DataFrame = pd.read_excel(\n",
    "    f\"{config.OUTPUT_FOLDER_BASE}observables/{config.DATASET_NAME}-distance-normalized-matrix-{config.DISTANCE_MEASURE_FINGERPRINT}-{config.NUMBER_OBSERVABLE_PATTERNS}.xlsx\",\n",
    "    index_col=0\n",
    ")\n",
    "features: List[str] = list(config.EXPLAINING_FEATURE_NAMES.keys())\n",
    "\n",
    "# Ensure that both datasets contain the same indices\n",
    "valid_indices = np.intersect1d(df_explainable.index, df_observable_distances.index)\n",
    "df_explainable = df_explainable.loc[valid_indices]\n",
    "df_observable_distances = df_observable_distances.loc[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def torch_correlation(u:torch.Tensor, v:torch.Tensor, w:torch.Tensor|None=None, centered:bool=False) -> torch.Tensor:\n",
    "\n",
    "    ######################################################################################\n",
    "    ### Conversion of the correlation distance from scipy.spatial.distance to pytorch. ###\n",
    "    ######################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the correlation distance between two 1-D arrays.\n",
    "\n",
    "    The correlation distance between `u` and `v`, is\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        1 - \\\\frac{(u - \\\\bar{u}) \\\\cdot (v - \\\\bar{v})}\n",
    "                  {{\\\\|(u - \\\\bar{u})\\\\|}_2 {\\\\|(v - \\\\bar{v})\\\\|}_2}\n",
    "\n",
    "    where :math:`\\\\bar{u}` is the mean of the elements of `u`\n",
    "    and :math:`x \\\\cdot y` is the dot product of :math:`x` and :math:`y`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : (N,) array_like\n",
    "        Input array.\n",
    "    v : (N,) array_like\n",
    "        Input array.\n",
    "    w : (N,) array_like, optional\n",
    "        The weights for each value in `u` and `v`. Default is None,\n",
    "        which gives each value a weight of 1.0\n",
    "    centered : bool, optional\n",
    "        If True, `u` and `v` will be centered. Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    correlation : double\n",
    "        The correlation distance between 1-D array `u` and `v`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Find the correlation between two arrays.\n",
    "\n",
    "    >>> from scipy.spatial.distance import correlation\n",
    "    >>> correlation([1, 0, 1], [1, 1, 0])\n",
    "    1.5\n",
    "\n",
    "    Using a weighting array, the correlation can be calculated as:\n",
    "\n",
    "    >>> correlation([1, 0, 1], [1, 1, 0], w=[0.9, 0.1, 0.1])\n",
    "    1.1\n",
    "\n",
    "    If centering is not needed, the correlation can be calculated as:\n",
    "\n",
    "    >>> correlation([1, 0, 1], [1, 1, 0], centered=False)\n",
    "    0.5\n",
    "    \"\"\"\n",
    "\n",
    "    if w is not None:\n",
    "        w = w\n",
    "        w = w / w.sum()\n",
    "    if centered:\n",
    "        if w is not None:\n",
    "            umu = torch.dot(u, w)\n",
    "            vmu = torch.dot(v, w)\n",
    "        else:\n",
    "            umu = torch.mean(u)\n",
    "            vmu = torch.mean(v)\n",
    "        u = u - umu\n",
    "        v = v - vmu\n",
    "    if w is not None:\n",
    "        vw = v * w\n",
    "        uw = u * w\n",
    "    else:\n",
    "        vw, uw = v, u\n",
    "    uv = torch.dot(u, vw)\n",
    "    uu = torch.dot(u, uw)\n",
    "    vv = torch.dot(v, vw)\n",
    "    dist = 1.0 - uv / torch.sqrt(uu * vv)\n",
    "    # Clip the result to avoid rounding error\n",
    "    return torch.clip(dist, 0.0, 2.0)\n",
    "\n",
    "@torch.jit.script\n",
    "def pearson_corrcoef(x:torch.Tensor, y:torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Calculates the Person correlation coefficient.\n",
    "    PyTorch equivalent to numpy.corrcoef(x, y)[0, 1].\n",
    "    '''\n",
    "\n",
    "    vx = x - torch.mean(x)\n",
    "    vy = y - torch.mean(y)\n",
    "    return torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "\n",
    "@torch.jit.script\n",
    "def relative_entropy(p, q):\n",
    "    return torch.special.entr(q) - torch.special.entr(p)\n",
    "\n",
    "@torch.jit.script\n",
    "def torch_jensenshannon(p:torch.Tensor, q:torch.Tensor):\n",
    "\n",
    "    ########################################################################################\n",
    "    ### Conversion of the jensenshannon distance from scipy.spatial.distance to pytorch. ###\n",
    "    ########################################################################################\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the Jensen-Shannon distance (metric) between\n",
    "    two probability arrays. This is the square root\n",
    "    of the Jensen-Shannon divergence.\n",
    "\n",
    "    The Jensen-Shannon distance between two probability\n",
    "    vectors `p` and `q` is defined as,\n",
    "\n",
    "    .. math::\n",
    "\n",
    "       \\\\sqrt{\\\\frac{D(p \\\\parallel m) + D(q \\\\parallel m)}{2}}\n",
    "\n",
    "    where :math:`m` is the pointwise mean of :math:`p` and :math:`q`\n",
    "    and :math:`D` is the Kullback-Leibler divergence.\n",
    "\n",
    "    This routine will normalize `p` and `q` if they don't sum to 1.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : (N,) array_like\n",
    "        left probability vector\n",
    "    q : (N,) array_like\n",
    "        right probability vector\n",
    "    base : double, optional\n",
    "        the base of the logarithm used to compute the output\n",
    "        if not given, then the routine uses the default base of\n",
    "        scipy.stats.entropy.\n",
    "    axis : int, optional\n",
    "        Axis along which the Jensen-Shannon distances are computed. The default\n",
    "        is 0.\n",
    "\n",
    "        .. versionadded:: 1.7.0\n",
    "    keepdims : bool, optional\n",
    "        If this is set to `True`, the reduced axes are left in the\n",
    "        result as dimensions with size one. With this option,\n",
    "        the result will broadcast correctly against the input array.\n",
    "        Default is False.\n",
    "\n",
    "        .. versionadded:: 1.7.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    js : double or ndarray\n",
    "        The Jensen-Shannon distances between `p` and `q` along the `axis`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    .. versionadded:: 1.2.0\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from scipy.spatial import distance\n",
    "    >>> import numpy as np\n",
    "    >>> distance.jensenshannon([1.0, 0.0, 0.0], [0.0, 1.0, 0.0], 2.0)\n",
    "    1.0\n",
    "    >>> distance.jensenshannon([1.0, 0.0], [0.5, 0.5])\n",
    "    0.46450140402245893\n",
    "    >>> distance.jensenshannon([1.0, 0.0, 0.0], [1.0, 0.0, 0.0])\n",
    "    0.0\n",
    "    >>> a = np.array([[1, 2, 3, 4],\n",
    "    ...               [5, 6, 7, 8],\n",
    "    ...               [9, 10, 11, 12]])\n",
    "    >>> b = np.array([[13, 14, 15, 16],\n",
    "    ...               [17, 18, 19, 20],\n",
    "    ...               [21, 22, 23, 24]])\n",
    "    >>> distance.jensenshannon(a, b, axis=0)\n",
    "    array([0.1954288, 0.1447697, 0.1138377, 0.0927636])\n",
    "    >>> distance.jensenshannon(a, b, axis=1)\n",
    "    array([0.1402339, 0.0399106, 0.0201815])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    p = p / torch.sum(p, dim=0)\n",
    "    q = q / torch.sum(q, dim=0)\n",
    "    \n",
    "    m = (p + q) / 2.0\n",
    "    left = relative_entropy(p, m)\n",
    "    right = relative_entropy(q, m)\n",
    "    left_sum = torch.sum(left, dim=0)\n",
    "    right_sum = torch.sum(right, dim=0)\n",
    "    js = left_sum + right_sum\n",
    "\n",
    "    return torch.sqrt(js / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import kl_divergence\n",
    "from torch.distributions.beta import Beta\n",
    "from ParameterizedModel.parameterized_model import Paremeterized_Model\n",
    "from ParameterizedModel.utils.distribution_helpers import mean_std_to_beta_params\n",
    "\n",
    "@torch.jit.script\n",
    "def _construct_distance_matrix(A:torch.Tensor, feature_weights:torch.Tensor, range:torch.Tensor) -> torch.Tensor:\n",
    "    distance_matrix =  torch.stack([\n",
    "        torch.stack([\n",
    "            torch_correlation(u = A[i], v = A[j], w = feature_weights)\n",
    "            for j in range]) for i in range\n",
    "    ])\n",
    "    return distance_matrix / distance_matrix.sum()\n",
    "\n",
    "class FeatureSelectionModel(Paremeterized_Model):\n",
    "    def __init__(self, df_explainable:pd.DataFrame, df_observable_distances:pd.DataFrame, num_samples:int=1, device='cpu', dtype=torch.float64):\n",
    "        super().__init__(device, dtype)\n",
    "\n",
    "        self.tensor_explainable = torch.tensor(df_explainable.to_numpy(), dtype=self.dtype, device=self.device)\n",
    "        self.tensor_observable_distances = torch.tensor(df_observable_distances.to_numpy(), dtype=self.dtype, device=self.device)\n",
    "\n",
    "        self.N = self.tensor_explainable.shape[0]\n",
    "        self.M = len(features)\n",
    "        self._range = torch.arange(self.N)\n",
    "\n",
    "        self.feature_names = list(df_explainable.columns)\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        alpha, beta = mean_std_to_beta_params(0.5, 0.05)\n",
    "        alpha_prior, beta_prior = mean_std_to_beta_params(0.001, 0.001)\n",
    "\n",
    "        noise_1 = torch.rand(len(features))\n",
    "        noise_2 = torch.rand(len(features))\n",
    "\n",
    "        alpha_init = alpha * torch.ones(len(features)) * 0.9 + noise_1 * 0.1\n",
    "        beta_init  = beta  * torch.ones(len(features)) * 0.9 + noise_2 * 0.1\n",
    "\n",
    "        self.add_param('alpha', torch.log(alpha_init), lambda x: x.exp())\n",
    "        self.add_param('beta',  torch.log(beta_init), lambda x: x.exp())\n",
    "\n",
    "        self.Beta_prior = Beta(\n",
    "            alpha_prior * torch.ones(len(features), dtype = self.dtype, device = self.device),\n",
    "            beta_prior  * torch.ones(len(features), dtype = self.dtype, device = self.device))\n",
    "        \n",
    "        self.best_loss = None \n",
    "        self.best_weights = None\n",
    "        \n",
    "    def get_feature_weights(self) -> dict:\n",
    "        #w = Beta(self.constrained('alpha'), self.constrained('beta')).mean.clone().detach().numpy()\n",
    "        w = self.best_feature_weights\n",
    "        feature_dict = {feature_name:float(weight) for feature_name, weight in zip(self.feature_names, w)}\n",
    "        return feature_dict\n",
    "    \n",
    "    def get_feature_certainty(self) -> dict:\n",
    "        s = Beta(self.constrained('alpha'), self.constrained('beta')).stddev.clone().detach().numpy()\n",
    "        stdev_dict = {feature_name:float(std) for feature_name, std in zip(self.feature_names, s)}\n",
    "        return stdev_dict\n",
    "    \n",
    "    def save_if_best_loss(self, loss, feature_weights):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss.clone().detach()\n",
    "            self.best_feature_weights = feature_weights.clone().detach()\n",
    "        elif loss < self.best_loss:\n",
    "            self.best_loss = loss.clone().detach()\n",
    "            self.best_feature_weights = feature_weights.clone().detach()\n",
    "\n",
    "    def sample(self, num_samples:int=1000):\n",
    "        samples = Beta(self.constrained('alpha'), self.constrained('beta')).sample([num_samples]).numpy()\n",
    "        res_df = pd.DataFrame({self.feature_names[i]:samples[:, i] for i in range(len(self.feature_names))})\n",
    "        return res_df\n",
    "    \n",
    "    def plot(self, num_samples:int=1000):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.barplot(self.sample(num_samples))\n",
    "\n",
    "    def loss(self, data:torch.Tensor|None=None) -> torch.tensor:\n",
    " \n",
    "        # Sample feature_weights\n",
    "        feature_weights = Beta(self.constrained('alpha'), self.constrained('beta')).rsample([self.num_samples]).mean(dim = 0)\n",
    "        #prior_weights = self.Beta_prior.mean #sample([self.num_samples]).mean\n",
    "      \n",
    "        # Construct the distance_matrix \n",
    "        distance_matrix = _construct_distance_matrix(A=self.tensor_explainable, feature_weights=feature_weights, range=self._range)\n",
    "\n",
    "        # Calculate the correlation between the distance matrix and the observable distances\n",
    "        coef = pearson_corrcoef(distance_matrix, self.tensor_observable_distances)\n",
    "\n",
    "        # Distance to the Prior\n",
    "        #dist = torch_jensenshannon(feature_weights, prior_weights) / self.N\n",
    "        #eucl_dist = (feature_weights - prior_weights).pow(2).sum().sqrt()\n",
    "        #dist = eucl_dist / self.N\n",
    "\n",
    "        # Loss\n",
    "        loss = -coef #+ dist\n",
    "\n",
    "        self.save_if_best_loss(loss, feature_weights)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ParameterizedModel.training import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeatureSelectionModel(df_explainable, df_observable_distances, num_samples=1)\n",
    "\n",
    "# After this many steps, the learning rate is halved\n",
    "HALFLIFE = 65\n",
    "\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    params = [\n",
    "        {'params': model.parameters['alpha'], 'lr': 0.1, 'momentum': 0.5},\n",
    "        {'params': model.parameters['beta'],  'lr': 0.1, 'momentum': 0.5},\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr_halflife = lambda epochs: 0.5**(1/epochs)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: lr_halflife(HALFLIFE)**epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model = model, optimizer = optimizer, scheduler = scheduler, dataloader = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 --- Mean Loss : -0.1732224697997422\n",
      "2/300 --- Mean Loss : -0.19337548031697627\n",
      "3/300 --- Mean Loss : -0.1974797499464454\n",
      "4/300 --- Mean Loss : -0.24459040793267028\n",
      "5/300 --- Mean Loss : -0.2583075858325248\n",
      "6/300 --- Mean Loss : -0.25863455241860883\n",
      "7/300 --- Mean Loss : -0.257970734461868\n",
      "8/300 --- Mean Loss : -0.2649561033973401\n",
      "9/300 --- Mean Loss : -0.2573249856931855\n",
      "10/300 --- Mean Loss : -0.2632235701892664\n",
      "11/300 --- Mean Loss : -0.2625631839503193\n",
      "12/300 --- Mean Loss : -0.2661854253674525\n",
      "13/300 --- Mean Loss : -0.2606973390221903\n",
      "14/300 --- Mean Loss : -0.2617787519953314\n",
      "15/300 --- Mean Loss : -0.26285700660336997\n",
      "16/300 --- Mean Loss : -0.2640077337197846\n",
      "17/300 --- Mean Loss : -0.26435426890588304\n",
      "18/300 --- Mean Loss : -0.26666061805031094\n",
      "19/300 --- Mean Loss : -0.2655596879349481\n",
      "20/300 --- Mean Loss : -0.26573296170608024\n",
      "21/300 --- Mean Loss : -0.266598764374786\n",
      "22/300 --- Mean Loss : -0.26357223402906177\n",
      "23/300 --- Mean Loss : -0.26623798787690733\n",
      "24/300 --- Mean Loss : -0.2660375523015509\n",
      "25/300 --- Mean Loss : -0.26588776906786715\n",
      "26/300 --- Mean Loss : -0.2661818883067446\n",
      "27/300 --- Mean Loss : -0.26359369425584867\n",
      "28/300 --- Mean Loss : -0.26537967383387234\n",
      "29/300 --- Mean Loss : -0.2668013402520405\n",
      "30/300 --- Mean Loss : -0.26653395428051374\n",
      "31/300 --- Mean Loss : -0.2638560310674819\n",
      "32/300 --- Mean Loss : -0.2647368288972972\n",
      "33/300 --- Mean Loss : -0.266685052397675\n",
      "34/300 --- Mean Loss : -0.26731686220185663\n",
      "35/300 --- Mean Loss : -0.26731804737478926\n",
      "36/300 --- Mean Loss : -0.2672784485220347\n",
      "37/300 --- Mean Loss : -0.2681118031758646\n",
      "38/300 --- Mean Loss : -0.2680590446354254\n",
      "39/300 --- Mean Loss : -0.26617988626057437\n",
      "40/300 --- Mean Loss : -0.2670993506031311\n",
      "41/300 --- Mean Loss : -0.2677600291951097\n",
      "42/300 --- Mean Loss : -0.2686251627612883\n",
      "43/300 --- Mean Loss : -0.2680326694550297\n",
      "44/300 --- Mean Loss : -0.26664817846880445\n",
      "45/300 --- Mean Loss : -0.2681813531172551\n",
      "46/300 --- Mean Loss : -0.26659647962691957\n",
      "47/300 --- Mean Loss : -0.2664069689913945\n",
      "48/300 --- Mean Loss : -0.2680433373086305\n",
      "49/300 --- Mean Loss : -0.2684457405784865\n",
      "50/300 --- Mean Loss : -0.2666752210598865\n",
      "51/300 --- Mean Loss : -0.26726797778429284\n",
      "52/300 --- Mean Loss : -0.26715870915785683\n",
      "53/300 --- Mean Loss : -0.26785523491635543\n",
      "54/300 --- Mean Loss : -0.2682710516987974\n",
      "55/300 --- Mean Loss : -0.268169593507813\n",
      "56/300 --- Mean Loss : -0.2670844212627705\n",
      "57/300 --- Mean Loss : -0.26833395183602615\n",
      "58/300 --- Mean Loss : -0.26829315792387104\n",
      "59/300 --- Mean Loss : -0.2680349714510486\n",
      "60/300 --- Mean Loss : -0.267996112976859\n",
      "61/300 --- Mean Loss : -0.2676113556221769\n",
      "62/300 --- Mean Loss : -0.2683912436689647\n",
      "63/300 --- Mean Loss : -0.26870077492520383\n",
      "64/300 --- Mean Loss : -0.2683281957813059\n",
      "65/300 --- Mean Loss : -0.26709372915204466\n",
      "66/300 --- Mean Loss : -0.2684353685673511\n",
      "67/300 --- Mean Loss : -0.26883887332619527\n",
      "68/300 --- Mean Loss : -0.26683594135007954\n",
      "69/300 --- Mean Loss : -0.26851715085835764\n",
      "70/300 --- Mean Loss : -0.2687779511332126\n",
      "71/300 --- Mean Loss : -0.26794478274337147\n",
      "72/300 --- Mean Loss : -0.2685003067204919\n",
      "73/300 --- Mean Loss : -0.26814705275165474\n",
      "74/300 --- Mean Loss : -0.26748206698119137\n",
      "75/300 --- Mean Loss : -0.26826036101661854\n",
      "76/300 --- Mean Loss : -0.2690099729181914\n",
      "77/300 --- Mean Loss : -0.2689747291594697\n",
      "78/300 --- Mean Loss : -0.26866759590902217\n",
      "79/300 --- Mean Loss : -0.26678743307630365\n",
      "80/300 --- Mean Loss : -0.2685468697819508\n",
      "81/300 --- Mean Loss : -0.26891443173371044\n",
      "82/300 --- Mean Loss : -0.2686954881595517\n",
      "83/300 --- Mean Loss : -0.2688526162736644\n",
      "84/300 --- Mean Loss : -0.26858096420330946\n",
      "85/300 --- Mean Loss : -0.26872924733671794\n",
      "86/300 --- Mean Loss : -0.26816802409978513\n",
      "87/300 --- Mean Loss : -0.2691217213707139\n",
      "88/300 --- Mean Loss : -0.26827026177830054\n",
      "89/300 --- Mean Loss : -0.2685188544548602\n",
      "90/300 --- Mean Loss : -0.2686486308821636\n",
      "91/300 --- Mean Loss : -0.269079257085742\n",
      "92/300 --- Mean Loss : -0.2679896070658025\n",
      "93/300 --- Mean Loss : -0.26803263803940214\n",
      "94/300 --- Mean Loss : -0.2685686222548249\n",
      "95/300 --- Mean Loss : -0.2686235167898817\n",
      "96/300 --- Mean Loss : -0.2685436310067482\n",
      "97/300 --- Mean Loss : -0.2684518154604\n",
      "98/300 --- Mean Loss : -0.2693185942390345\n",
      "99/300 --- Mean Loss : -0.26912491030847235\n",
      "100/300 --- Mean Loss : -0.2682089440945676\n",
      "101/300 --- Mean Loss : -0.2694055185713345\n",
      "102/300 --- Mean Loss : -0.2690403828057874\n",
      "103/300 --- Mean Loss : -0.2688067582372915\n",
      "104/300 --- Mean Loss : -0.2679566983089187\n",
      "105/300 --- Mean Loss : -0.2688293003562038\n",
      "106/300 --- Mean Loss : -0.2683514092073557\n",
      "107/300 --- Mean Loss : -0.26916192699368063\n",
      "108/300 --- Mean Loss : -0.2683873552171644\n",
      "109/300 --- Mean Loss : -0.26941792908547135\n",
      "110/300 --- Mean Loss : -0.2688381091010069\n",
      "111/300 --- Mean Loss : -0.2687053483474669\n",
      "112/300 --- Mean Loss : -0.2692993132234116\n",
      "113/300 --- Mean Loss : -0.2683372947277165\n",
      "114/300 --- Mean Loss : -0.2688202106756954\n",
      "115/300 --- Mean Loss : -0.26846607598212113\n",
      "116/300 --- Mean Loss : -0.2677975735218972\n",
      "117/300 --- Mean Loss : -0.26900766092341183\n",
      "118/300 --- Mean Loss : -0.2688321510072672\n",
      "119/300 --- Mean Loss : -0.26856256707454956\n",
      "120/300 --- Mean Loss : -0.2693131050421682\n",
      "121/300 --- Mean Loss : -0.26857566458737614\n",
      "122/300 --- Mean Loss : -0.26886175349812585\n",
      "123/300 --- Mean Loss : -0.2693608513517337\n",
      "124/300 --- Mean Loss : -0.26874405329772133\n",
      "125/300 --- Mean Loss : -0.2692473760745531\n",
      "126/300 --- Mean Loss : -0.2691639908498683\n",
      "127/300 --- Mean Loss : -0.26940400534562964\n",
      "128/300 --- Mean Loss : -0.2686971641212432\n",
      "129/300 --- Mean Loss : -0.2693239791131005\n",
      "130/300 --- Mean Loss : -0.26907930742788927\n",
      "131/300 --- Mean Loss : -0.26925437623237747\n",
      "132/300 --- Mean Loss : -0.26910383979297675\n",
      "133/300 --- Mean Loss : -0.2693806900906851\n",
      "134/300 --- Mean Loss : -0.2695443026455263\n",
      "135/300 --- Mean Loss : -0.2683477073228319\n",
      "136/300 --- Mean Loss : -0.2685393874160978\n",
      "137/300 --- Mean Loss : -0.2692703926349259\n",
      "138/300 --- Mean Loss : -0.2693526730166451\n",
      "139/300 --- Mean Loss : -0.26894583689088725\n",
      "140/300 --- Mean Loss : -0.2689368954255375\n",
      "141/300 --- Mean Loss : -0.2691396275543984\n",
      "142/300 --- Mean Loss : -0.2693919381248132\n",
      "143/300 --- Mean Loss : -0.26897755071080465\n",
      "144/300 --- Mean Loss : -0.26943351080296646\n",
      "145/300 --- Mean Loss : -0.2688650363133438\n",
      "146/300 --- Mean Loss : -0.26917237665278\n",
      "147/300 --- Mean Loss : -0.2694270363224697\n",
      "148/300 --- Mean Loss : -0.26907029667803745\n",
      "149/300 --- Mean Loss : -0.2693478277493029\n",
      "150/300 --- Mean Loss : -0.2693276157291724\n",
      "151/300 --- Mean Loss : -0.2688037439090044\n",
      "152/300 --- Mean Loss : -0.26941829691988534\n",
      "153/300 --- Mean Loss : -0.269236028822269\n",
      "154/300 --- Mean Loss : -0.26923542716323967\n",
      "155/300 --- Mean Loss : -0.2691228653265581\n",
      "156/300 --- Mean Loss : -0.26921299912858593\n",
      "157/300 --- Mean Loss : -0.26957660622961327\n",
      "158/300 --- Mean Loss : -0.2693648982706981\n",
      "159/300 --- Mean Loss : -0.2684452467287605\n",
      "160/300 --- Mean Loss : -0.26926459979879236\n",
      "161/300 --- Mean Loss : -0.2684819124153811\n",
      "162/300 --- Mean Loss : -0.2692753269669792\n",
      "163/300 --- Mean Loss : -0.26918815486909325\n",
      "164/300 --- Mean Loss : -0.2694514950771885\n",
      "165/300 --- Mean Loss : -0.26954944166178263\n",
      "166/300 --- Mean Loss : -0.26927895689626236\n",
      "167/300 --- Mean Loss : -0.2696300673108987\n",
      "168/300 --- Mean Loss : -0.2696547139261264\n",
      "169/300 --- Mean Loss : -0.2694276210018451\n",
      "170/300 --- Mean Loss : -0.26899937704428284\n",
      "171/300 --- Mean Loss : -0.2694403998913755\n",
      "172/300 --- Mean Loss : -0.26866486633740605\n",
      "173/300 --- Mean Loss : -0.2688569265405945\n",
      "174/300 --- Mean Loss : -0.26934185227730456\n",
      "175/300 --- Mean Loss : -0.26933586520026526\n",
      "176/300 --- Mean Loss : -0.2695321623673251\n",
      "177/300 --- Mean Loss : -0.2693648795897414\n",
      "178/300 --- Mean Loss : -0.2694320852901695\n",
      "179/300 --- Mean Loss : -0.26847088373626754\n",
      "180/300 --- Mean Loss : -0.2693620482400039\n",
      "181/300 --- Mean Loss : -0.2694320216825234\n",
      "182/300 --- Mean Loss : -0.2690182908517135\n",
      "183/300 --- Mean Loss : -0.26887848083371313\n",
      "184/300 --- Mean Loss : -0.2695839296889484\n",
      "185/300 --- Mean Loss : -0.2693087197756589\n",
      "186/300 --- Mean Loss : -0.2692800738339182\n",
      "187/300 --- Mean Loss : -0.26937250176926075\n",
      "188/300 --- Mean Loss : -0.2692717624965327\n",
      "189/300 --- Mean Loss : -0.26920419613949575\n",
      "190/300 --- Mean Loss : -0.2696488338236216\n",
      "191/300 --- Mean Loss : -0.26943795275684873\n",
      "192/300 --- Mean Loss : -0.2693083828798484\n",
      "193/300 --- Mean Loss : -0.26931535831517045\n",
      "194/300 --- Mean Loss : -0.2689764360054635\n",
      "195/300 --- Mean Loss : -0.26893008083909203\n",
      "196/300 --- Mean Loss : -0.2694918008163636\n",
      "197/300 --- Mean Loss : -0.2694340741309583\n",
      "198/300 --- Mean Loss : -0.269110988777122\n",
      "199/300 --- Mean Loss : -0.26911826697205277\n",
      "200/300 --- Mean Loss : -0.2695256502848195\n",
      "201/300 --- Mean Loss : -0.269193856303422\n",
      "202/300 --- Mean Loss : -0.26901242136605363\n",
      "203/300 --- Mean Loss : -0.2694024704954685\n",
      "204/300 --- Mean Loss : -0.2688608133449839\n",
      "205/300 --- Mean Loss : -0.2694374066480603\n",
      "206/300 --- Mean Loss : -0.26944419982891865\n",
      "207/300 --- Mean Loss : -0.2694662501052794\n",
      "208/300 --- Mean Loss : -0.26958733982525224\n",
      "209/300 --- Mean Loss : -0.2694272569347338\n",
      "210/300 --- Mean Loss : -0.26954961260755766\n",
      "211/300 --- Mean Loss : -0.26929870945381035\n",
      "212/300 --- Mean Loss : -0.26966126878913554\n",
      "213/300 --- Mean Loss : -0.26944028502587136\n",
      "214/300 --- Mean Loss : -0.26951947998761494\n",
      "215/300 --- Mean Loss : -0.26945845541824753\n",
      "216/300 --- Mean Loss : -0.26936047364930005\n",
      "217/300 --- Mean Loss : -0.2695617724473031\n",
      "218/300 --- Mean Loss : -0.26955829353464106\n",
      "219/300 --- Mean Loss : -0.269568804308421\n",
      "220/300 --- Mean Loss : -0.269480952763728\n",
      "221/300 --- Mean Loss : -0.26959974881212073\n",
      "222/300 --- Mean Loss : -0.2693337070684148\n",
      "223/300 --- Mean Loss : -0.26931698868039355\n",
      "224/300 --- Mean Loss : -0.2693455997049677\n",
      "225/300 --- Mean Loss : -0.2692937789400912\n",
      "226/300 --- Mean Loss : -0.2691653788079106\n",
      "227/300 --- Mean Loss : -0.26914383611121306\n",
      "228/300 --- Mean Loss : -0.2695839739506647\n",
      "229/300 --- Mean Loss : -0.2694404557301189\n",
      "230/300 --- Mean Loss : -0.2693297641851979\n",
      "231/300 --- Mean Loss : -0.2696677807828216\n",
      "232/300 --- Mean Loss : -0.2695754597162915\n",
      "233/300 --- Mean Loss : -0.2692909793474775\n",
      "234/300 --- Mean Loss : -0.2691991102213071\n",
      "235/300 --- Mean Loss : -0.26899291435161937\n",
      "236/300 --- Mean Loss : -0.2694668088439665\n",
      "237/300 --- Mean Loss : -0.26949793039302516\n",
      "238/300 --- Mean Loss : -0.2693821126228457\n",
      "239/300 --- Mean Loss : -0.2695323901184584\n",
      "240/300 --- Mean Loss : -0.2694840322791087\n",
      "241/300 --- Mean Loss : -0.26948817737515407\n",
      "242/300 --- Mean Loss : -0.2688601842619467\n",
      "243/300 --- Mean Loss : -0.2695851302022447\n",
      "244/300 --- Mean Loss : -0.2692476001368853\n",
      "245/300 --- Mean Loss : -0.26946885038652657\n",
      "246/300 --- Mean Loss : -0.26960460769074296\n",
      "247/300 --- Mean Loss : -0.26951956384241516\n",
      "248/300 --- Mean Loss : -0.2690931318046829\n",
      "249/300 --- Mean Loss : -0.26948411611433964\n",
      "250/300 --- Mean Loss : -0.2695666806222942\n",
      "251/300 --- Mean Loss : -0.26952446515858663\n",
      "252/300 --- Mean Loss : -0.26960738079646274\n",
      "253/300 --- Mean Loss : -0.2690750767537031\n",
      "254/300 --- Mean Loss : -0.2693139528242914\n",
      "255/300 --- Mean Loss : -0.2695622945263259\n",
      "256/300 --- Mean Loss : -0.2695655234687666\n",
      "257/300 --- Mean Loss : -0.26966238198367043\n",
      "258/300 --- Mean Loss : -0.269446959172664\n",
      "259/300 --- Mean Loss : -0.26926842454146205\n",
      "260/300 --- Mean Loss : -0.269426725006531\n",
      "261/300 --- Mean Loss : -0.2696503826555885\n",
      "262/300 --- Mean Loss : -0.2692212162894288\n",
      "263/300 --- Mean Loss : -0.26958154249748784\n",
      "264/300 --- Mean Loss : -0.2693519107343228\n",
      "265/300 --- Mean Loss : -0.2693823857289394\n",
      "266/300 --- Mean Loss : -0.26951843526220387\n",
      "267/300 --- Mean Loss : -0.269504978854701\n",
      "268/300 --- Mean Loss : -0.2693242501819076\n",
      "269/300 --- Mean Loss : -0.2693592302926561\n",
      "270/300 --- Mean Loss : -0.269553297364303\n",
      "271/300 --- Mean Loss : -0.2694901401859072\n",
      "272/300 --- Mean Loss : -0.2696170808479778\n",
      "273/300 --- Mean Loss : -0.26953985812519865\n",
      "274/300 --- Mean Loss : -0.269067947114364\n",
      "275/300 --- Mean Loss : -0.26933759910671107\n",
      "276/300 --- Mean Loss : -0.2696346097817803\n",
      "277/300 --- Mean Loss : -0.2695486623353376\n",
      "278/300 --- Mean Loss : -0.2693330829685315\n",
      "279/300 --- Mean Loss : -0.26949242878846497\n",
      "280/300 --- Mean Loss : -0.269513756465198\n",
      "281/300 --- Mean Loss : -0.2694524165780344\n",
      "282/300 --- Mean Loss : -0.2694487159863195\n",
      "283/300 --- Mean Loss : -0.2693854607365966\n",
      "284/300 --- Mean Loss : -0.2695141111233037\n",
      "285/300 --- Mean Loss : -0.26956509003851115\n",
      "286/300 --- Mean Loss : -0.2695621379596263\n",
      "287/300 --- Mean Loss : -0.2694198953950734\n",
      "288/300 --- Mean Loss : -0.26943060004353\n",
      "289/300 --- Mean Loss : -0.269598040716362\n",
      "290/300 --- Mean Loss : -0.2695173808572956\n",
      "291/300 --- Mean Loss : -0.26939497507506904\n",
      "292/300 --- Mean Loss : -0.26939257826645796\n",
      "293/300 --- Mean Loss : -0.26941882763715336\n",
      "294/300 --- Mean Loss : -0.2691651959368804\n",
      "295/300 --- Mean Loss : -0.2694639047509245\n",
      "296/300 --- Mean Loss : -0.26921791088531205\n",
      "297/300 --- Mean Loss : -0.26948001113061953\n",
      "298/300 --- Mean Loss : -0.2696768487997807\n",
      "299/300 --- Mean Loss : -0.26918220901621387\n",
      "300/300 --- Mean Loss : -0.2695983732680836\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epochs = 300, timeout=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2698136823712533"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(model.best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
